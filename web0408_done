import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import time
from datetime import datetime
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
from urllib.request import urlopen
from bs4 import BeautifulSoup
import csv
import requests


pages = []
f = csv.writer(open('z-artist-names.csv', 'w'))
f.writerow(['Name', 'Link'])

for i in range(1,5):
    url = "https://web.archive.org/web/20121007172955/http://www.nga.gov/collection/anZ"+str(i)+".htm"
    pages.append(url)

for i in pages:
    html = urlopen(i).read()
    soup = BeautifulSoup(html,'html.parser')
    last_links = soup.find(class_='AlphaNav')
    last_links.decompose()
    artist_name_list = soup.find(class_='BodyText')
    artist_name_list_items = artist_name_list.find_all('a')
    #print(artist_name_list_items)
    for artist_name in artist_name_list_items:
        names = artist_name.contents[0]
        links = 'https://web.archive.org' + artist_name.get('href')
        f.writerow([names,links])

print("done")
